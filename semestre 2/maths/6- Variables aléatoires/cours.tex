%%=====================================================================================
%%
%%       Filename:  cours.tex
%%
%%    Description:  
%%
%%        Version:  1.0
%%        Created:  03/06/2024
%%       Revision:  none
%%
%%         Author:  YOUR NAME (), 
%%   Organization:  
%%      Copyright:  Copyright (c) 2024, YOUR NAME
%%
%%          Notes:  
%%
%%=====================================================================================
\documentclass[a4paper, titlepage]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[french]{babel}
\usepackage{amsmath, amssymb}
\usepackage{amsthm}
\usepackage[svgnames]{xcolor}
\usepackage{thmtools}
\usepackage{lipsum}
\usepackage{framed}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage{newtxtext}

% \renewcommand{\familydefault}{\sfdefault}

% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\newcommand{\incfig}[1]{%
	\def\svgwidth{\columnwidth}
	\import{./figures/}{#1.pdf_tex}
}

\pdfsuppresswarningpagegroup=1

\colorlet{defn-color}{DarkBlue}
\colorlet{props-color}{Blue}
\colorlet{warn-color}{Red}
\colorlet{exemple-color}{Green}
\colorlet{corol-color}{Orange}
\newenvironment{defn-leftbar}{%
  \def\FrameCommand{{\color{defn-color}\vrule width 3pt} \hspace{10pt}}%
  \MakeFramed {\advance\hsize-\width \FrameRestore}}%
 {\endMakeFramed}
\newenvironment{warn-leftbar}{%
  \def\FrameCommand{{\color{warn-color}\vrule width 3pt} \hspace{10pt}}%
  \MakeFramed {\advance\hsize-\width \FrameRestore}}%
 {\endMakeFramed}
\newenvironment{exemple-leftbar}{%
  \def\FrameCommand{{\color{exemple-color}\vrule width 3pt} \hspace{10pt}}%
  \MakeFramed {\advance\hsize-\width \FrameRestore}}%
 {\endMakeFramed}
\newenvironment{props-leftbar}{%
  \def\FrameCommand{{\color{props-color}\vrule width 3pt} \hspace{10pt}}%
  \MakeFramed {\advance\hsize-\width \FrameRestore}}%
 {\endMakeFramed}
\newenvironment{corol-leftbar}{%
  \def\FrameCommand{{\color{corol-color}\vrule width 3pt} \hspace{10pt}}%
  \MakeFramed {\advance\hsize-\width \FrameRestore}}%
 {\endMakeFramed}

\def \freespace {1em}
\declaretheoremstyle[headfont=\sffamily\bfseries,%
 notefont=\sffamily\bfseries,%
 notebraces={}{},%
 headpunct=,%
% bodyfont=\sffamily,%
 headformat=\color{defn-color}Définition~\NUMBER\hfill\NOTE\smallskip\linebreak,%
 preheadhook=\vspace{\freespace}\begin{defn-leftbar},%
 postfoothook=\end{defn-leftbar},%
]{better-defn}
\declaretheoremstyle[headfont=\sffamily\bfseries,%
 notefont=\sffamily\bfseries,%
 notebraces={}{},%
 headpunct=,%
% bodyfont=\sffamily,%
 headformat=\color{warn-color}Attention\hfill\NOTE\smallskip\linebreak,%
 preheadhook=\vspace{\freespace}\begin{warn-leftbar},%
 postfoothook=\end{warn-leftbar},%
]{better-warn}
\declaretheoremstyle[headfont=\sffamily\bfseries,%
 notefont=\sffamily\bfseries,%
notebraces={}{},%
headpunct=,%
% bodyfont=\sffamily,%
 headformat=\color{exemple-color}Exemple~\NUMBER\hfill\NOTE\smallskip\linebreak,%
 preheadhook=\vspace{\freespace}\begin{exemple-leftbar},%
 postfoothook=\end{exemple-leftbar},%
]{better-exemple}
\declaretheoremstyle[headfont=\sffamily\bfseries,%
 notefont=\sffamily\bfseries,%
 notebraces={}{},%
 headpunct=,%
% bodyfont=\sffamily,%
 headformat=\color{props-color}Proposition~\NUMBER\hfill\NOTE\smallskip\linebreak,%
 preheadhook=\vspace{\freespace}\begin{props-leftbar},%
 postfoothook=\end{props-leftbar},%
]{better-props}
\declaretheoremstyle[headfont=\sffamily\bfseries,%
 notefont=\sffamily\bfseries,%
 notebraces={}{},%
 headpunct=,%
% bodyfont=\sffamily,%
 headformat=\color{props-color}Théorème~\NUMBER\hfill\NOTE\smallskip\linebreak,%
 preheadhook=\vspace{\freespace}\begin{props-leftbar},%
 postfoothook=\end{props-leftbar},%
]{better-thm}
\declaretheoremstyle[headfont=\sffamily\bfseries,%
 notefont=\sffamily\bfseries,%
 notebraces={}{},%
 headpunct=,%
% bodyfont=\sffamily,%
 headformat=\color{corol-color}Corollaire~\NUMBER\hfill\NOTE\smallskip\linebreak,%
 preheadhook=\vspace{\freespace}\begin{corol-leftbar},%
 postfoothook=\end{corol-leftbar},%
]{better-corol}

\declaretheorem[style=better-defn]{defn}
\declaretheorem[style=better-warn]{warn}
\declaretheorem[style=better-exemple]{exemple}
\declaretheorem[style=better-corol]{corol}
\declaretheorem[style=better-props, numberwithin=defn]{props}
\declaretheorem[style=better-thm, sibling=props]{thm}
\newtheorem*{lemme}{Lemme}%[subsection]
%\newtheorem{props}{Propriétés}[defn]

\newenvironment{system}%
{\left\lbrace\begin{align}}%
{\end{align}\right.}

\newenvironment{AQT}{{\fontfamily{qbk}\selectfont AQT}}

\usepackage{LobsterTwo}
\titleformat{\section}{\newpage\LobsterTwo \huge\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\vspace{2em}\LobsterTwo \Large\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\vspace{1em}\LobsterTwo \large\bfseries}{\thesubsubsection.}{1em}{}

\newenvironment{lititle}%
{\vspace{7mm}\LobsterTwo \large}%
{\\}

\renewenvironment{proof}{$\square$ \footnotesize\textit{Démonstration.}}{\begin{flushright}$\blacksquare$\end{flushright}}

\title{Variables aléatoires}
\author{William Hergès\thanks{Sorbonne Université - Faculté des Sciences, Faculté des Lettres}}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{Variables aléatoires discrètes}
	Souvent il est très compliqué de déterminer une loi de probabilité. On introduit donc les variables aléatoires pour régler ce problème.

	Dans le cas d'un lancer de dé, on a :
	$$ \Omega = \{(i,j)|(i,j)\in\{1,\ldots,6\}^2\} = \{1,\ldots,6\}^2 $$
	Ça donne beaucoup de possibilités, on va donc introduire la notion de variables aléatoires pour résoudre ce problème.

	\subsection{Définitions}
	\begin{defn}[Ensemble dénombrable]
		Soit $D$ un ensemble.

		On dit que $D$ est dénombrable si et seulement si~:
		\begin{itemize}
			\item il existe une bijection entre $\mathbb{N}$ et $D$
			\item ou il est fini (son cardinal est différent de $\infty$)
		\end{itemize}
	\end{defn}

	\begin{defn}[Variable aléatoire discrète]
		Soit $(\Omega,\mathbb{P})$ un ensemble probabilisé et $D$ un ensemble dénombrable.

		Alors, $X$ est défini tel que~:
		$$ X:\Omega \to D $$
	\end{defn}
	$X$ est donc une fonction.

	\begin{defn}
		La loi de probabilité $Q$ de $X$ est définie telle que~:
		$$\begin{matrix}
			\mathcal{P}(D) &\to &[0;1]\\
			A & \longmapsto & \mathbb{P}(X^{-1}(A)) = \mathbb{P}(X\in A)
		\end{matrix}$$
	\end{defn}
	$(D,Q)$ forme un espace probabilisé.
	
	Écrire $X\in A$ est étrange, car cela veut dire que $X$, une application, appartient à n'importe quelle ensemble.

	On a donc que $Q$ est une nouvelle probabilité fonctionnant comme les autres.
	Il suffit donc de connaître $Q(\{K\})$ pour tout $k$ dans $D$ pour pouvoir déterminer la variable aléatoire.

	\begin{props}
		On a~:
		$$ Q(A) = \mathbb{P}(X\in A) = \sum_{k\in A} \mathbb{P}(X=k) $$
	\end{props}
	On se réduit donc à connaître la probabilité que $X=k$, ce que l'on note $p_k$.

	On a~:
	$$ \sum_{k\in D} p_k = 1 $$

	\begin{exemple}
		Si on reprend l'exemple du dé en introduction, on a que la probabilité d'avoir $p_2$ (c'est-à-dire que la somme de $i+j$ vaut $2$) est de $1/36$.
	\end{exemple}

	\subsection{Lois usuelles}
	\begin{defn}
		On dit que $X$ suit la loi uniforme si, et seulement si, $X$ ne prend qu'une unique valeur.

		On note~:
		$$ X\sim\mathcal{U}(n) $$
		où $n$ représente le nombre de valeur prise par $X$.
	\end{defn}

	\begin{defn}
		On dit que $X$ suit la loi de Bernoulli si, et seulement si, $X$ ne prend que les valeurs $0$ et $1$ et que $p$ est la réussite, alors $q$, l'échec, vaut $1-p$.

		On note~:
		$$ X\sim\mathcal{B}(p) $$
	\end{defn}

	\begin{exemple}
		Soit $\Omega = \{0, 1, 2, 3\}$ et $D = \{0, 1\}$ (on a donc que $D\subseteq \Omega$) où $\mathbb{P}(0\cup 1) = 1$.\\
		Donc, $p_0+p_1 = 1$ et $p_0 = 1- p_1$ car $p_0\in[0;1]$.\\
		On a donc que $X:\Omega \to D$ suit la loi de Bernoulli. Si $p_1 = 1/2$, alors on dira que $X$ suit une loi uniforme (et que $p_0 = p_1$).
	\end{exemple}
	Pour dire que $X$ suit la loi de Bernoulli, on écrit $\mathcal{B}(p)$.
	Pour dire que $X$ suit la loi uniforme, on écrit $\mathcal{U}(2)$ (d'une manière générale, $2$ est remplaçable par $n\in\mathbb{N}^*$).

	\begin{defn}
		On dit que $X$ suit la loi binomiale si, et seulement si, elle est composée d'une somme de variable aléatoire $(X_n)$, où $n$ est fixé, suivant la loi de Bernoulli de paramètre $p$ fixé.

		On note~:
		$$ X\sim\mathcal{B}(n,p) $$
	\end{defn}
	On a que $X$ est le nombre de succès rencontré après avoir rencontré $n$ épreuves de Bernoulli de probabilité de succès $p$.

	\begin{props}
		On a que pour tous les $p_k$ de $X$ d'une variable aléatoire $X$ suivant une loi binomiale de paramètre $n$ et $p$~:
		$$ \forall k\in D,\quad p_k = \mathcal{C}^k_n p^k(1-p)^{n-k} $$
	\end{props}
	\begin{proof}
		On a~:
		$$ \forall k\in D,\quad p_k = \mathcal{C}^k_n p^k(1-p)^{n-k} $$
		D'après le binôme de Newton, on a~:
		$$ \sum_{k\in D} p_k = (p-(p-1))^{n} = 1 $$
	\end{proof}

	\begin{defn}
		On dit que $X$ suit la loi de Poisson si, et seulement si, pour $\lambda\in\mathbb{R}^*_+$ fixé et pour tout $k\in D$ (où $D=\mathbb{N}$ ici), on a~:
		$$ p_k = \frac{\lambda^k}{k!}e^{-\lambda} $$

		On note~:
		$$ X\sim\mathcal{P}(\lambda) $$
	\end{defn}
	\begin{proof}
		On a~:
		$$ \sum_{k\in\mathbb{N}} p_k = \sum_{k\in\mathbb{N}} \frac{\lambda^k}{k!}e^{-\lambda} = e^{-\lambda}\sum_{k\in\mathbb{N}} \frac{\lambda^k}{k!} = e^{-\lambda}e^{\lambda} = 1 $$
		par définition de $\exp\{\lambda\}$.
	\end{proof}
	
	\subsection{Espérance et variance}
	\begin{defn}[Espérance]
		On définit l'espérance de $X$ par~:
		$$ \mathrm{E}(X) = \sum_{k\in D} kp_k $$
	\end{defn}
	Permet de calculer ce qu'on peut espérer de $X$.

	\begin{defn}[Variance]
		On définit la variance de $X$ par~:
		$$ \sigma^2(X) = \mathrm{Var}(X) = \sum_{k\in D} (k-E(X))^2p_k $$
	\end{defn}
	Permet de mesurer à quel point on peut s'écarter de l'espérance.

	\begin{props}
		On a~:
		$$ \mathrm{Var}(X) = E(X^2) - E(X)^2 $$
	\end{props}

	\begin{thm}
		Si $X$ suit la loi uniforme de paramètre $n$ avec $D=[1,n]$, alors~:
		$$ \mathrm{E}(X) = \frac{n+1}{2} $$
		et
		$$ \mathrm{Var}(X) = \frac{n^2-1}{12} $$
	\end{thm}

	\begin{thm}
		Si $X$ suit la loi de Bernoulli de paramètre $p$, alors~:
		$$ \mathrm{E}(X) = p $$
		et
		$$ \mathrm{Var}(X) = p(1-p) $$
	\end{thm}

	\begin{thm}
		Si $X$ suit la loi binomiale de paramètre $n$ et $p$, alors~:
		$$ \mathrm{E}(X) = np $$
		et
		$$ \mathrm{Var}(X) = np(1-p) $$
	\end{thm}

	\begin{thm}
		Si $X$ suit la loi de Poisson de paramètre $\lambda$, alors~:
		$$ \mathrm{E}(X) = \lambda $$
		et
		$$ \mathrm{Var}(X) = \lambda $$
	\end{thm}
	\section{Variables aléatoires à densité}
	Soit $(\Omega,\mathbb{P})$ un espace probabilisé.

	On s'est vite restreint aux probabilités $\mathbb{P}(X^{-1}(A))$, où $A\in\mathcal{P}(D)$ avec $D$ un un ensemble dénombrable et $X$ est de $D$ dans $\Omega$, que l'on a noté $\mathbb{P}(X\in A)$.

	Si $D$ est dénombrable, on a que~:
	$$ \sum_{k\in D} p_k = 1 $$
	où $p_k$ est $\mathbb{P}(X=k)$.
	Si $D$ n'était pas dénombrable, le symbole somme n'aurait aucun sens~!
	\subsection{Définitions}
	\begin{defn}[Variable aléatoire à densité]
		Une variable aléatoire est une fonction de $\Omega$ dans $E$, un ensemble pas forcément dénombrable.

		Ainsi, $\mathbb{P}(X\in A)$, on a que $A$ est bien souvent une partie de $\mathbb{R}$.
		On peut donc s'intéresser aux intervalles du style $[a,b]$ où $(a,b)\in E^2$.

		Cette variable est à densité s'il existe une fonction $f$ croissante telle que~:
		$$ \mathbb{P}(X\in[a,b]) = \int^b_a f(t)\mathrm{d}t $$
		On appelle $f$ la densité de probabilité.
	\end{defn}

	\begin{defn}[Fonction de densité]
		On appelle $F$ la fonction de densité de $X$ définie telle que~:
		$$ \forall t\in E,\quad F(t)=\int^{t}_{-\infty} f(s)\mathrm{d}s $$
		où $f$ est la fonction associée à $X$.
	\end{defn}

	\begin{props}
		On a donc que~:
		$$ \mathbb{P}(X\in[a,b]) = \mathbb{P}(a\leqslant X\leqslant b) = \mathbb{P}(X\leqslant b) - \mathbb{P}(X\geqslant a) $$
	\end{props}

	\begin{props}[Propriétés de la fonction de densité]
		Les propriétés de $F$ sont~:
		\begin{itemize}
			\item  sa croissance
			\item $0\leqslant F\leqslant 1$
			\item $\displaystyle\int^{+\infty}_{-\infty}f(t)\mathrm{d}t = 1$, i.e. $\displaystyle\lim_{t \to \infty} F(t) = 1$
		\end{itemize}
	\end{props}
	\begin{proof}
		\AQT
	\end{proof}
	Ces propriétés sont celles analogues pour les variables aléatoires à~:
	$$ \sum_{k\in D} p_k = 1 $$
	des variables aléatoires discrètes.

	\subsection{Lois usuelles}
	\begin{defn}
		On dit que $X$ suit la loi uniforme de paramètre $[a,b]$ si~:
		$$ f(x) =
		\left\{\begin{matrix}
				\frac{1}{b-a}&\text{si}~x\in[a,b]\\
				0 &\text{sinon}
		\end{matrix}\right. $$
		On note~:
		$$ X\sim\mathcal{U} ([a,b]) $$
	\end{defn}
	La fonction de répartition dans ce cas est (si $x\in[a,b]$)~:
	$$ \int_a^x \frac{1}{b-a}\mathrm{d}t = \frac{x-a}{b-a} $$

	\begin{defn}
		On dit que $X$ suit la loi exponentielle de paramètre $\lambda > 0$ si~:
		$$ f(x) = \left\{\begin{matrix} \lambda \exp\left( -\lambda x \right) &\text{si}~x\geqslant 0\\ 0 &\text{sinon} \end{matrix}\right. $$
		On note~:
		$$ X\sim\mathcal{E}(\lambda) $$
	\end{defn}
	La fonction de répartition dans ce cas est~:
	$$ \int_0^x \lambda\exp\left\{ -\lambda t \right\} \mathrm{d}t = 1-\exp\left\{ -\lambda x \right\}  $$

	\begin{defn}
		On dit que $X$ suit la loi normale de paramètres $\mu$, son espérence, et $\sigma$, son écart type, si pour tout $x$ on a ~:
		$$ f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left\{ -\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2 \right\}  $$
		où $f$ désigne la densité de probabilité de $X$.

		On note souvent~:
		$$ X\sim\mathcal{N}(\mu,\sigma^2) $$
		où $\sigma^2$ représente la variance
	\end{defn}
	Attention, $\sigma$ est toujours strictement supérieur à 0~!

	La fonction de répartition de $\mathcal{N}(0,1)$ est~:
	$$ \frac{1}{\sqrt{2\pi}}\int^x_{-\infty} \exp\left\{ -\frac{1}{2}t^2 \right\} \mathrm{d}t $$
	\subsection{Espérance et variance}
	\begin{defn}[Espérance]
		Si $X$ est une variable aléatoire à densité $f$, alors~:
		$$ \mathrm{E}(X) = \int^{+\infty}_{-\infty} tf(t)\mathrm{d}t $$
	\end{defn}

	\begin{defn}[Variance]
		Si $X$ est une variable aléatoire à densité $f$, alors~:
		$$ \mathrm{Var}(x) = E(X^2)-E(X)^2 $$
		ce qui vaut
		$$ \int^{+\infty}_{-\infty}(t-E(X))^2f(t)\mathrm{d}t $$
	\end{defn}

	\begin{thm}
		Si $X$ suit la loi uniforme de paramètre $[a,b]$, alors~:
		$$ \mathrm{E}(X) = \frac{b+a}{2} $$
		et
		$$ \mathrm{Var}(X) = \frac{(b-a)^2}{12} $$
	\end{thm}
	\begin{proof}
		\AQT
	\end{proof}

	\begin{thm}
		Si $X$ suit la loi exponentielle de paramètre $\lambda > 0$, alors :
		$$ E(X) = \frac{1}{\lambda} $$
		et
		$$ \mathrm{Var}(X) = \frac{1}{\lambda^2} $$
	\end{thm}

	\begin{thm}
		Si $X$ suit la loi normale de paramètre $m,\sigma^2$, alors :
		$$ E(X) = m $$
		et
		$$ \mathrm{Var}(X) = \sigma^2 $$
	\end{thm}

	\subsection{Indépendance et suites de variables}
	\begin{thm}[Inégalité de Markov]
		Si $X$ est une variable aléatoire réelle \textit{positive} telle que $E(X)$ est bien définie, on a~:
		$$ \forall r>0,\quad \mathbb{P}(X\geqslant r)\leqslant \frac{E(X)}{r} $$
	\end{thm}
	Cette preuve est importante niveau notation~!

	\begin{proof}
		Notons $g$ la fonction définie par $g(x) = r$ si $x \geqslant r$ et $g(x) = 0$ sinon. $g$ est une variable aléatoire discrète. Alors~:
		\begin{itemize}
			\item $\{g(x)=r\} = \left\{ X\geqslant r \right\} $
			\item $\left\{ g(x) = 0 \right\} = \left\{ X < r \right\}$
		\end{itemize}
		On a aussi que $g(x) \leqslant x$ pour tout $x$ positif, ce qui nous donne~:
		$$ E(X) \geqslant E(g(x)) = 0\times\mathbb{P}(X < r) + r\mathbb{P}(X \geqslant r) $$
		par croissance de l'espérance. Ainsi,
		$$ \mathbb{P}(X\geqslant r) \leqslant \frac{1}{r}E(X) $$
	\end{proof}

	\begin{props}[Inégalité de Bienaymé-Tchebychev]
		Si $X$ est une variable aléatoire réelle telle que $E(X)$ et $\mathrm{Var}(X)$ sont bien définies, alors~:
		$$ \forall r>0,\quad \mathbb{P}(|X-E(X)| \geqslant r)\leqslant \frac{\mathrm{Var}(X)}{r^2} $$
	\end{props}
	\begin{proof}
		Cette formule découle de l'inégalité de Markov~: il s'agit du changement de variable $Z=(X-E(X))^2$ qui donne $(X-E(X))^2 \geqslant r^2$, d'où le résultat.
	\end{proof}

	\begin{defn}
		On dit que deux variables $X$ et $Y$ sont indépendantes si pour toutes fonctions $f$ et $g$ on a~:
		$$ E(f(X)g(Y)) = E(f(X))E(g(Y)) $$
		sous réserve d'espérence bien définie.
	\end{defn}

	\begin{thm}
		Si $X$ et $Y$ sont deux variables réelles indépendantes, on a~:
		$$ \mathrm{Var}(X+Y) = \mathrm{Var}(X)+\mathrm{Var}(Y) $$
	\end{thm}
	La bonne définition des variances est assurées dans ce cas~!

	\begin{thm}[Loi faible des grands nombres]
		Soit $(X_k)_{k\in\mathbb{N}}$ une suite de variables aléatoires réelles telle que~:
		$$ \forall k\in \mathbb{N},\quad E(X_k) = E(X_0)\quad\land\quad\mathrm{Var}(X_k) = \mathrm{Var}(X_0) $$
		et que toutes les variables sont indépendantes deux à deux. Alors~:
		$$ \forall \varepsilon > 0,\quad \mathbb{P}\left( \left| \frac{1}{n}\sum_{i=1}^{n} X_i-E(X_0) \right| \geqslant \varepsilon \right) \xrightarrow[n \to \infty]{} 0 $$
	\end{thm}
	On l'appelle aussi LGN.

	\begin{thm}[Théorème central limite]
		On prend une suite respectant les mêmes conditions que celles de la LGN. Pour tout réels $a<b$, on a~:
		$$ \mathbb{P}\left(\frac{\sqrt n}{\sqrt{\mathrm{Var}(X_0)}} \left(\frac{1}{n}\sum_{i=1}^{n} X_i-E(X_0) \right) \in [a,b] \right) \xrightarrow[n \to \infty]{} \frac{1}{\sqrt{2 \pi}}\int^b_a \exp\left\{ -\frac{x^2}{2} \right\} \mathrm{d}x $$
	\end{thm}
	C'est le théorème qui est central, pas la limite. Donc, \textbf{pas de e à central}~!

	On a donc que pour un grand nombre de variables, leur somme est équivalente à une variable aléatoire $Y\sim\mathcal{N}(0,1)$ (dite gaussienne centrée réduite).
	En effet, la somme pour un $n$ assez grand est similaire à la fonction de répartition de la loi normale.
\end{document}
